{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asaju\\Desktop\\Intent-Classification\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset silicone (C:/Users/asaju/.cache/huggingface/datasets/silicone/maptask/1.0.0/af617406c94e3f78da85f7ea74ebfbd3f297a9665cb54adbae305b03bc4442a5)\n",
      "100%|██████████| 3/3 [00:00<00:00, 178.56it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"silicone\", \"maptask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the dataset into train val and test\n",
    "train_dataset = dataset[\"train\"]\n",
    "val_dataset = dataset[\"validation\"]\n",
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "# Separate X (Utterance) and y (Dialogue_Act)\n",
    "train_X = train_dataset[\"Utterance\"]\n",
    "train_y = train_dataset[\"Dialogue_Act\"]\n",
    "\n",
    "val_X = val_dataset[\"Utterance\"]\n",
    "val_y = val_dataset[\"Dialogue_Act\"]\n",
    "\n",
    "test_X = test_dataset[\"Utterance\"]\n",
    "test_y = test_dataset[\"Dialogue_Act\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize the data\n",
    "train_encodings = tokenizer(train_X, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_X, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_X, truncation=True, padding=True)\n",
    "\n",
    "# Convert labels from string to one hot\n",
    "label_list = np.unique(train_y)\n",
    "label_dict = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "train_y_one_hot = np.zeros((len(train_y), len(label_list)))\n",
    "val_y_one_hot = np.zeros((len(val_y), len(label_list)))\n",
    "test_y_one_hot = np.zeros((len(test_y), len(label_list)))\n",
    "\n",
    "for i, label in enumerate(train_y):\n",
    "    train_y_one_hot[i][label_dict[label]] = 1\n",
    "\n",
    "for i, label in enumerate(val_y):\n",
    "    val_y_one_hot[i][label_dict[label]] = 1\n",
    "\n",
    "for i, label in enumerate(test_y):\n",
    "    test_y_one_hot[i][label_dict[label]] = 1\n",
    "\n",
    "# Create the dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    train_y_one_hot\n",
    "))\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(val_encodings),\n",
    "    val_y_one_hot\n",
    "))\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(test_encodings),\n",
    "    test_y_one_hot\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 12 class lstm model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(tokenizer.vocab_size, 256),\n",
    "    tf.keras.layers.LSTM(256, return_sequences=True),\n",
    "    tf.keras.layers.LSTM(256),\n",
    "    tf.keras.layers.Dense(12, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=[tf.keras.metrics.CategoricalAccuracy()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asaju\\Desktop\\Intent-Classification\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:592: UserWarning: Input dict contained keys ['input_ids', 'token_type_ids', 'attention_mask'] which did not match any model input. They will be ignored by the model.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 28s 318ms/step - loss: 2.4717 - categorical_accuracy: 0.1418 - val_loss: 2.4221 - val_categorical_accuracy: 0.1792\n",
      "Epoch 2/10\n",
      "21/21 [==============================] - 5s 247ms/step - loss: 2.3810 - categorical_accuracy: 0.2072 - val_loss: 2.3790 - val_categorical_accuracy: 0.1792\n",
      "Epoch 3/10\n",
      "21/21 [==============================] - 5s 247ms/step - loss: 2.3307 - categorical_accuracy: 0.2068 - val_loss: 2.3408 - val_categorical_accuracy: 0.1792\n",
      "Epoch 4/10\n",
      "21/21 [==============================] - 5s 243ms/step - loss: 2.3179 - categorical_accuracy: 0.2067 - val_loss: 2.3437 - val_categorical_accuracy: 0.1792\n",
      "Epoch 5/10\n",
      "21/21 [==============================] - 5s 243ms/step - loss: 2.3157 - categorical_accuracy: 0.2064 - val_loss: 2.3445 - val_categorical_accuracy: 0.1792\n",
      "Epoch 6/10\n",
      "21/21 [==============================] - 5s 244ms/step - loss: 2.3172 - categorical_accuracy: 0.2072 - val_loss: 2.3440 - val_categorical_accuracy: 0.1792\n",
      "Epoch 7/10\n",
      "21/21 [==============================] - 5s 246ms/step - loss: 2.3180 - categorical_accuracy: 0.2062 - val_loss: 2.3442 - val_categorical_accuracy: 0.1792\n",
      "Epoch 8/10\n",
      "21/21 [==============================] - 5s 246ms/step - loss: 2.3188 - categorical_accuracy: 0.2063 - val_loss: 2.3444 - val_categorical_accuracy: 0.1792\n",
      "Epoch 9/10\n",
      "21/21 [==============================] - 5s 248ms/step - loss: 2.3190 - categorical_accuracy: 0.2055 - val_loss: 2.3446 - val_categorical_accuracy: 0.1792\n",
      "Epoch 10/10\n",
      "21/21 [==============================] - 5s 246ms/step - loss: 2.3176 - categorical_accuracy: 0.2065 - val_loss: 2.3443 - val_categorical_accuracy: 0.1792\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "batch_size = 1024\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset.shuffle(1000).batch(batch_size),\n",
    "    epochs=10,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=val_dataset.batch(batch_size)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
